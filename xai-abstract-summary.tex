\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{array}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Explainable Artificial Intelligence \\ {Methodology for Handwritten Applications}}

\author{\IEEEauthorblockN{Paul Whitten, Francis Wolff, Chris Papachristou}
\IEEEauthorblockA{\textit{Electrical, Computer, and Systems Engineering} \\
\textit{Case School of Engineering} \\
\textit{Case Western Reserve University} \\
Cleveland, OH, USA \\
pcw@case.edu, fxw12@case.edu, cap2@case.edu}

}

\maketitle

\begin{abstract}
There has been explosive growth of practical AI in recent years.
%However, inferential results of AI systems are not readily explainable to humans.
A major concern of current AI systems is an inability to explain these inferential decisions.
%Explainable artificial intelligence has been posed to mitigate these concerns.
This work explores an Explainable Artificial Intelligence (XAI) methodology that provides explanations
for classification decisions.  Experimental results using the MNIST handwritten digit database are provided with explainable conclusions.
\end{abstract}

\begin{IEEEkeywords}
explainable, artificial intelligence, machine learning
\end{IEEEkeywords}

\section{Introduction}

Recent advances in Machine Learning (ML) have brought about wide adoption of ML algorithms for many applications.  Despite various successes, there is a reluctance to adopt ML in some applications because ML behaves like a black box, decision making by the black box is often not explainable to humans.  

This work approaches the widely studied problem of classifying images of handwritten digits into the ten decimal digit classes, zero through nine, from an Explainable Artificial Intelligence (XAI) perspective.

While we approach XAI for a specific classification problem in the MNIST handwritten digit database, which consists of 28x28 images of handwritten digits into the ten decimal digits, we feel the methodology translates to other problems of explainable classification among a finite set of classes.

This work aims to pose a means of explaining the classification to a human.  We do not wish to compete with established algorithms that perform exceptionally well in classification of input \cite{keysers07} \cite{lecun98} \cite{schm2012}.  The effort required in applying the methodology in this work is significant compared to training a classifier that will act as a black box, and therefore not be explainable.

\section{Related work}

The ability to map the learning classifier or recognizer to human-based explainability is a challenging task for human understandability.  Currently, there are at least seventeen explainable techniques such as
decision tree-based, rule-based (i.e. knowledge-base), salience mapping,
sensitivity-based analysis, feature importance, fuzzy-based, neural-network, and generic-programming based.  These techniques use one of three basic evaluation approaches: application-grounded, human-grounded and functionally grounded. \cite{BlackBox18} \cite{Survey18} \cite{Fuzzy19} \cite{Hagras18}  \cite{GP18}.

Distributed and fault tolerant systems research has provided several examples of voting \cite{avizienis} and probabilistic models for the voting problem \cite{blough}.

\section{Summary}

 \begin{figure}[htbp]
\centerline{\includegraphics[width=95mm]{./images/voting_prop_nn_2.png}}
\caption{XAI Architecture Summary}
\label{voting}
\end{figure}

Our methodology for explainability is summarized as the use of explainable properties and related transformations of the input to make distinct classification decisions for each property.  Those distinct classifications are input to a voter to provide the best classification result.  Rationale, based on the explainable properties, are combined with the voter result to provide an explanation. 

The methodology is depicted in Fig.~\ref{voting} where $I_i$ represents the $i$th input.  The $P_x$ squares represent logic including distinct neural network architectures to make classifications based on properties where $x$ is 1 through n.   $Q_i$ represents the $i$th property's classification output.  The Voter Model Engine is then used to consider each of the choices, $Q_i$, from the properties and provide the best decision.  Based on property results an explanation is assembled in XAI.   The explanation consists of the explainable properties that contribute to the classification.   The decision along with the property-based explanation is presented to a user.

Further detailing explainable properties, we considered how explainable properties could be mapped to the input.  In this work, we pose the use of input transformations that are related to explainable properties to aid in classification while providing human understandable rationale for classification decisions.  We chose to use one transformation per property in this work.  However, each property may have multiple such transformations.  This is represented in Fig.~\ref{proptrans} where the outer square represents the $i$th property square from Fig.~\ref{voting}.  The boxes labeled $T_x$ indicate the $x$th transformation of the input related to that property.  The transformed input is fed to a Neural Network Architecture (NNA) to make classification decisions based on the transformed input.  Output from the property transform decisions then flow to the voter as shown in Fig.~\ref{voting}.

 \begin{figure}[htbp]
\centerline{\includegraphics[width=90mm]{./images/property_transforms.png}}
\caption{Architecture of a single property for multiple transformations.}
\label{proptrans}
\end{figure}

This work explores two voting model schemes.  One voting scheme is probabilistic while the second involves an ML algorithm.  Regardless of the voting algorithm, it was useful to construct and utilize a knowledgebase for storing detailed training and test results of the property NNAs.  The knowledgebase was also important in estimating confidence used in voting algorithms. 
 
Our methodology for explainability involves the following steps:
\begin{itemize}
\item Discover explainable properties.
\item Define transformations for explainable properties.
\item Transform training data.
\item Produce trained explainable property-specific NNAs.
\item Build a Knowledgebase across the explainable properties.
\item Devise a voting scheme.
\item Use a test dataset to provide feedback.
\end{itemize}

\subsection{Explainable Properties}
The initial step is to discover explainable properties.  An explainable property is an attribute of a sample in the problem domain that may differentiate classes and provide a rationale for classification.  In the MNIST handwritten digit database example, we pose explainable properties such as the stroke, circle, ellipse, lines, endpoints, and crossings.

\subsection{Transformations}
Data transformations are next defined and implemented to represent a sample for classification according to the initial step's explainable properties.  Transforms may be known algorithms of feature detection and extraction that represent property characteristics.  In the MNIST example, we used digital image processing techniques related to the properties for transformations.  

\subsection{Training Data} 
We next generate a transformed training dataset by submitting all elements from the original training set to the property transformations.  The output from each property transformation is stored separately for training each property-specific NNA. 

\subsection{Trained Property NNAs}
The next step involves initializing unique NNAs representing each property and then using supervised ML techniques to train the NNAs using only the output of that particular property transform and the original labels.  This results in trained NNAs for each property.

\subsection{Knowledgebase}
After training, we again process the training set and populate a knowledgebase.  The knowledgebase stores the original training label, and each property's classification results from the property NNA.  The knowledgebase is used in constructing the voting strategy. 

\subsection{Voting}
We next devise a voting strategy using information from the knowledgebase.  The purpose for the voting strategy is to select among the multiple and potentially conflicting votes from the properties.

The first voting scheme we present is probabilistic.  In this scheme, we use the knowledgebase to identify each property NNA's effectiveness to correctly predict a digit class.   The effectiveness $E_{\pi,\gamma}$ for a property, $\pi$, to select a particular class, $\gamma$,  is obtained from the knowledgebase and is given by $E_{\pi,\gamma}$ and is equal to the ratio of the number of correct classifications of property $\pi$ of the class $\gamma$ to the total number of elements of class $\gamma$.   We take the sum of effectiveness metrics, for the properties that select a class $(\gamma)$, to obtain a weight $W_\gamma$ for that class.  The confidence for the class $\gamma$ is given by $C_\gamma=\frac{W_\gamma}{\sum\limits_iW_i}$.  The denominator is the sum of all weights for classes that were selected.  If multiple classes are selected by the properties, the class with the highest confidence is will be chosen by the voter.

The second classification scheme utilizes the knowledgebase as a set to train a NNA to select a digit on property transform results.  Using this scheme, we show very good results.  In this scheme we do not produce a confidence metric from the Knowledgebase but can present each digit's value form the voting NNA as confidence.

\subsection{feedback}
Finally, when test data is presented to the XAI architecture, we evaluate the results and determine if they are sufficient or if more properties or transforms are needed.

\section{Results}

In this work, we utilized the well-studied  MNIST handwritten digit database to develop the methodology.  Results obtained using the probabilistic voting engine were $91.9\%$ while results obtained from the NN voting engine were $95.9\%$.  We present details of experiments here.

While earlier figures have been of a general nature, Fig.~\ref{transsample} and following figures are specific to the application of the XAI methodology to the MNIST dataset.   The nine properties and corresponding transformations used for the MNIST database are shown in Fig.~\ref{transsample}.  The figure shows some sample original digits and corresponding transforms.

\bgroup
\renewcommand{\arraystretch}{1.8}
%\setlength\tabcolsep{2mm}
\begin{figure}
\centering
\begin{tabular}{ c | p{0.23\linewidth} | p{0.23\linewidth} | ccc | }
\cline{2-6}
& Property $P_i$ & Transform & $I_i$ &  &  $T_i$ \\
\hline \hline
$P_1$ & Stroke & Skeleton & \raisebox{-.5\height}{\includegraphics[width=8mm]{./digit-images/4-11.png}} & $\rightarrow$ & \raisebox{-.5\height}{\includegraphics[width=8mm]{./digit-images/4-11-skel.png}} \\
\hline
$P_2$ & Circle & Hough Circle & \raisebox{-.5\height}{\includegraphics[width=8mm]{./digit-images/6-17.png}} & $\rightarrow$ & \raisebox{-.5\height}{\includegraphics[width=8mm]{./digit-images/6-17-circle.png}} \\
\hline
$P_3$ & Crossings & Crossings & \raisebox{-.5\height}{\includegraphics[width=8mm]{./digit-images/4-2.png}} & $\rightarrow$ & \raisebox{-.5\height}{\includegraphics[width=8mm]{./digit-images/4-2-crossing.png}} \\
\hline
$P_4$ & Circle & Hough Ellipse & \raisebox{-.5\height}{\includegraphics[width=8mm]{./digit-images/0-3.png}} & $\rightarrow$ & \raisebox{-.5\height}{\includegraphics[width=8mm]{./digit-images/0-3-ellipse.png}} \\
\hline
$P_5$ & Circle & Multiple Ellipse Circle & \raisebox{-.5\height}{\includegraphics[width=8mm]{./digit-images/8-4.png}} & $\rightarrow$ & \raisebox{-.5\height}{\includegraphics[width=8mm]{./digit-images/8-4-ellipse-circle.png}} \\
\hline
$P_6$ & Endpoints & Endpoints & \raisebox{-.5\height}{\includegraphics[width=8mm]{./digit-images/2-2.png}} & $\rightarrow$ & \raisebox{-.5\height}{\includegraphics[width=8mm]{./digit-images/2-2-endpoint.png}} \\
\hline
$P_7$ & Enclosed Region & Flood Fill & \raisebox{-.5\height}{\includegraphics[width=8mm]{./digit-images/0-2.png}} & $\rightarrow$ & \raisebox{-.5\height}{\includegraphics[width=8mm]{./digit-images/0-2-fill.png}} \\
\hline
$P_8$ & Line & Hough Line & \raisebox{-.5\height}{\includegraphics[width=8mm]{./digit-images/7-20.png}} & $\rightarrow$ & \raisebox{-.5\height}{\includegraphics[width=8mm]{./digit-images/7-20-line.png}} \\
\hline
$P_9$ & Enclosed Stroke & Skeleton Flood Fill & \raisebox{-.5\height}{\includegraphics[width=8mm]{./digit-images/8-3.png}} & $\rightarrow$ & \raisebox{-.5\height}{\includegraphics[width=8mm]{./digit-images/8-3-skel-fill.png}} \\
\hline
\end{tabular}
\centering
\caption{Properties and transforms for the MNIST example}
\label{transsample}
\end{figure}
\egroup

The Stroke property $P_1$ is meant to minimally represent the digit, removing the variability of the line thickness among samples.  The morphological skeleton,  transformation $T_1$, is a one pixel connected representation preserving the topology of the digit and providing the representation of the stroke.  We utilized the Lee\cite{Lee1994} algorithm for obtaining skeletons of the MNIST handwritten digits.

$P_2$ is the circle property with the corresponding transform $T_2$ the Hough Circle.  $P_3$ is the crossings property, representing the intersection of lines in a digit.  The corresponding transform, $T_3$, for finding crossings involved taking the skeleton and then finding activated pixels with more than two neighbors.  The ellipse property, $P_4$m involved a transform, $T_4$, taking the Hough Ellipse of a digit.  $P_5$ for multiple circle or ellipse involved a transform, $T_5$, where Hough Ellipse and Hough Circle regions were compared to find non-overlapping combinations.   The endpoint property, $P_6$ involved a transform, $T_6$, where the skeleton was taken and then activated pixels with only one digit were identified.  Property $P_7$ for an enclosed region involved transform $T_7$ involved a flood fill that does not result in edge pixels.  The line property, $P_8$, involves transform $T_8$ which takes non-overlapping Hough Lines of a digit.  Finally, property $P_9$ which considers an enclosed section of a stroke takes the flood fill of a skeleton.

Note that the endpoints and crossings shown in Fig.~\ref{transsample} have a single pixel, representing each endpoint or crossing, activated and neighboring pixels partially activated to aid in classification.

Tables ~\ref{table:digit5out} and ~\ref{table:digit6out} show the NNA outputs and statistics for properties for digits file and six.  These tables represent the results from processing all of the digits labeled five and six respectively.   The digit rows in the tables represent the means of the NNA outputs for each digit.  The first line of the tables represents effectiveness, $E_{\pi,\gamma}$.  The second, third and fourth rows represent the standard deviation, kurtosis, and skew of the digit outputs for each property.  The last two rows are the false positive and false negative rates.

We observe that the Stroke property ($P_0$) performed very well with 100\% accuracy in both digits.  The next highest performing property in both tables was the endpoint property ($P_5$) with 85.4\% and 93.3\% accuracy.  The digit six also had good classification results for the fill properties ($P_6$ and $P_8$) due to enclosed regions in the digit six.  The digit six also performed better than the five in the properties related to the circle and ellipse ($P_2$, $P_3$, and $P_4$).

The digit five had among the poorest performance as observed by the relatively low percent correct and high false negative rates in Table. ~\ref{table:digit5out}.  The digit one also had notably poor performance for circle and ellipse related properties.  Obviously, this was due to the lack of a curve in a one.

The mean digit values from the Property NNAs are also represented  in Fig.~\ref{digit5votes} and ~\ref{digit6votes} as three dimensional surface plots for digits five and  and six.

\begin{figure}[htbp]
\centerline{\includegraphics[width=95mm]{./images/digit-5.png}}
\caption{Mean property NNA output for the digit 5}
\label{digit5votes}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=95mm]{./images/digit-6.png}}
\caption{Mean property NNA output for the digit 6}
\label{digit6votes}
\end{figure}

%\begin{table}[htbp]
%\caption{Properties, Transforms and Property Identifiers}
%\centering
%\begin{tabular}{| c | c | c |}
%\hline
% Identifier & Explainable Property & Transform \\
%\hline\hline
%$P_1$ & Stroke & Skeleton \\
%\hline
%$P_x$ & Circle & Hough Circle \\
%\hline
%$P_x$ & Crossings & Crossing Point \\
%\hline
%$P_x$ & Ellipse & Hough Ellipse \\
%\hline
%$P_x$ & Ellipse + Circle & Hough Ellipse and Circle \\
%\hline
%$P_x$ & Endpoints & Endpoints \\
%\hline
%$P_x$ & Enclosed Region & Flood Fill \\
%\hline
%$P_x$ & Line & Hough Line \\
%\hline
%$P_x$ & Enclosed Region of Skeleton & Skeleton Flood Fill \\
%\hline
%\end{tabular}
%\label{table:tblproptrans}
%\end{table}

 \begin{figure}[htbp]
\centerline{\includegraphics[width=15mm]{./digit-images/5-0.png}}
\caption{Example 1 of a handwritten digit five}
\label{example1}
\end{figure}

We present three examples of digits with the property classifications and explainable rationale and explain the vote based on the probabilistic voting scheme.

\begin{table}[htbp]
\caption{Probabilistic voting and explainability for Example 1}
\centering
\begin{tabular}{| c | c | c | c | p{0.08\linewidth} | p{0.08\linewidth} |}
\hline
 Prop. & Vote & $E_{\pi,5}$ & $E_{\pi,6}$ & $X_5$ & $X_6$ \\
\hline \cline{0-5}
$P_1$ & 5 & 1.000 & - & \checkmark & - \\ 
\hline
$P_2$ & 6 & - & 0.465 & - & \checkmark \\
\hline
$P_3$ & - & - &  - & - & - \\
\hline
$P_4$ & - & - & - & - & - \\
\hline
$P_5$ & 6 & - & 0.490 & - & \checkmark \\
\hline
$P_6$ & 5 & 0.854 & - & \checkmark & - \\
\hline
$P_7$ & - & - & - & - & - \\
\hline
$P_8$ & 5 & 0.700 & - & \checkmark & - \\
\hline
$P_9$ & - & - & - & - & - \\
\hline \cline{0-5}
\multicolumn{2}{|c|}{Weight Totals} & $2.554$ & $0.955$ & \multicolumn{2}{c|}{$\sum W_\gamma=3.509$} \\
\cline{0-5}
\multicolumn{2}{|c|}{Confidence} & $73\%$ & $24\%$ & \multicolumn{2}{c}{} \\
\cline{0-3}
\end{tabular}
\label{table:example1}
\end{table}

The first example  digit, labeled a five, is shown in Fig.~\ref{example1}.  Voting and explainability is detailed in Table ~\ref{table:example1}.  The property votes, corresponding to $Q_i$ in Fig.~\ref{voting}, for this example are shown in the Vote column.  Cells without a vote are properties that did not have a sufficiently strong opinion on a particular digit, i.e., no class was above a threshold. for a property  The $E_{\iota,x}$ columns give the effectiveness of a property $\iota$ to correctly select class $x$.  The effectiveness values are from the knowledgebase.   The stroke $(P_1)$, endpoint $(P_6)$ and line $(P_8)$ properties suggest the digit is a five with effectiveness $E_{1,5}= 1.000$, $E_{6,5}=0.854$, and $E_{8,5}=0.700$ summing to a weight $W_5=2.554$.  The circle $(P_2)$ and ellipse + circle properties $(P_5)$ suggest that the digit is a six with effectiveness $E_{2,6}=0.465$ and $E_{5,6}=0.490$  summing to a weight of $W_6=0.955$.  Taking the sum of the total weights, $\sum\limits_\gamma W_\gamma=3.509$.  Confidence for the five, is given as $C_5=\frac{2.554}{3.509} = 73\%$.  Alternatively, six was suggested by properties but not chosen by the voter because confidence is $C_6=\frac{0.955}{3.509}=27\% < 73\%$.  We also observe that the explainability, $X_x$, columns provide rationale for each of the classes that were selected based on the selecting properties.   E.g.,  the column $X_5$ indicates that the five was selected because the stroke, endpoint, and line properties are consistent with a five.   We see in $X_6$ that the six was suggested because circle and ellipse + circle properties were consistent with those of a six. 

 \begin{figure}[htbp]
\centerline{\includegraphics[width=15mm]{./digit-images/9-9.png}}
\caption{Example 2 of a handwritten digit nine}
\label{example2}
\end{figure}

%\begin{table}[htbp]
%\caption{Property Votes for Example 2}
%\centering
%\begin{tabular}{| c | c | c |}
%\hline
% Property Id & Vote & Weight \\
%\hline\hline
%$P_0$ & 9 & 1.000 \\ 
%\hline
%$P_1$ & 3 & 0.327 \\
%\hline
%$P_2$ & 5 & 0.057 \\
%\hline
%$P_3$ & - & - \\
%\hline
%$P_4$ & - & - \\
%\hline
%$P_5$ & 6 & 0.932 \\
%\hline
%$P_6$ & 9 & 0.809 \\
%\hline
%$P_7$ & - & - \\
%\hline
%$P_8$ & 9 & 0.821 \\
%\hline
%\end{tabular}
%\label{table:example2}
%\end{table}

The second example handwritten digit,  labeled a nine, is shown in Fig.~\ref{example2}.  Table ~\ref{table:example2} shows the property predictions, voting results, and rationale for example 3.  In this example three properties selected the digit nine and three other properties selected the digits three, five, and six.  The results from the voter for this example were that nine was selected by the voter with a $67\%$ confidence and an explanation that the stroke,  enclosed region, and enclosed region of the stroke were consistent with those of a nine.

 \begin{figure}[htbp]
\centerline{\includegraphics[width=15mm]{./digit-images/2-4.png}}
\caption{Example 3 of a handwritten digit two}
\label{example3}
\end{figure}

%\begin{table}[htbp]
%\caption{Property Votes for Example 3}
%\centering
%\begin{tabular}{| c | c | c |}
%\hline
% Property Id & Vote & Weight \\
%\hline\hline
%$P_0$ & 2 & 1.000 \\ 
%\hline
%$P_1$ & 3 & 0.327 \\
%\hline
%$P_2$ & - & - \\
%\hline
%$P_3$ & 2 & 0.161 \\
%\hline
%$P_4$ & 3 & 0.387 \\
%\hline
%$P_5$ & 2 & 0.938 \\
%\hline
%$P_6$ & - & - \\
%\hline
%$P_7$ & 2 & 0.639 \\
%\hline
%$P_8$ & - & - \\
%\hline
%\end{tabular}
%\label{table:example3}
%\end{table}

\begin{table}[htbp]
\caption{Probabilistic voting and explainability for Example 3}
\centering
\begin{tabular}{| c | c | c | c | p{0.08\linewidth} | p{0.08\linewidth} |}
\hline
 Prop. & Vote & $E_{\pi,52}$ & $E_{\pi,3}$ & $X_2$ & $X_3$ \\
\hline \cline{0-5}
$P_1$ & 2 & 1.000 & - & \checkmark & - \\ 
\hline
$P_2$ & 3 & - & 0.327 & - & \checkmark \\
\hline
$P_3$ & - & - &  - & - & - \\
\hline
$P_4$ & 2 & 0.161 & - & \checkmark & - \\
\hline
$P_5$ & 3 & - & 0.387 & - & \checkmark \\
\hline
$P_6$ & 2 & 0.938 & - & \checkmark & - \\
\hline
$P_7$ & - & - & - & - & - \\
\hline
$P_8$ & 2 & 0.639 & - & \checkmark & - \\
\hline
$P_9$ & - & - & - & - & - \\
\hline \cline{0-5}
\multicolumn{2}{|c|}{Weight Totals} & $2.738$ & $0.714$ & \multicolumn{2}{c|}{$\sum W_\gamma=3.452$} \\
\cline{0-5}
\multicolumn{2}{|c|}{Confidence} & $79\%$ & $21\%$ & \multicolumn{2}{c}{} \\
\cline{0-3}
\end{tabular}
\label{table:example3}
\end{table}

The third example handwritten digit, labeled a two, is shown in Fig. ~\ref{example3} and Table ~\ref{table:example3}.  In this case,  the voter selects the digit two with a $79\%$ confidence due to stroke, ellipse, endpoint, and line properties consistent with the digit two.  A three was suggested with $21\%$ confidence due to circle and ellipse circle properties.

\begin{table}[htbp]
\caption{NNA Voter results for examples}
\centering
\begin{tabular}{| c | c | c | c |}
\hline
 Digit & Ex. 1 & Ex. 2 & Ex. 3 \\
\hline\hline
0 & 1.92e-06 & 1.11e-08 & 2.98e-07\\ 
\hline
1 & 1.44e-06 & 1.52e-07 & 1.93e-08 \\
\hline
2 & 1.40e-06 & 4.54e-09 & \textbf{9.99e-01} \\
\hline
3 & 5.12e-08 & 2.48e-04 & 5.26e-06 \\
\hline
4 & 1.79e-08 & 1.33e-06 & 5.91e-06 \\
\hline
5 & \textbf{9.99e-01} & 2.31e-07 & 1.41e-06 \\
\hline
6 & 7.69e-07 & 2.23e-04 & 5.83e-07 \\
\hline
7 & 4.88e-07 & 4.93e-08 & 1.89e-09 \\
\hline
8 & 2.47e-07 & 5.71e-06 & 5.51e-07 \\
\hline
9 & 5.21e-07 & \textbf{9.99e-01} & 1.71e-08 \\
\hline
\end{tabular}
\label{table:nnavoter}
\end{table}

Table~\ref{table:nnavoter} shows the results of the ML Voting scheme on the three examples.  The ten rows in the table represent the corresponding digits.  The columns for examples 1 through 3 contain the values output by the NNA when presented with the property votes from the examples.  We observe that in each example the ML Voter overwhelmingly selects the appropriate digit shown in bold.

\begin{table*}[htbp]
\caption{Probabilistic voting and explainability for Example 2}
\centering
\begin{tabular}{| c | c | c | c | c | c | c | c | c | c |}
\hline
 Prop. & Vote & $E_{\pi,3}$ & $E_{\pi,5}$ & $E_{\pi,6}$ & $E_{\pi,9}$ & $X_3$ & $X_5$ & $X_6$ & $X_9$ \\
\hline \cline{0-9}
$P_1$ & 9 & - & - & - & 1.00 & - & - & - & \checkmark \\ 
\hline
$P_2$ & 3 & 0.327 & - & - & - & \checkmark & - & - & - \\
\hline
$P_3$ & 5 & - &  0.057 & - & - & - & \checkmark & - & - \\
\hline
$P_4$ & - & - & - & - & - & - & - & - & - \\
\hline
$P_5$ & - & - & - & - & - & - & - & - & - \\
\hline
$P_6$ & 6 & - & - & 0.933 & - & - & - & \checkmark & - \\
\hline
$P_7$ & 9 & - & - & - & 0.809 & - & - & - & \checkmark \\
\hline
$P_8$ & - & - & - & - & - & - & - & - & - \\
\hline
$P_9$ & 9 & - & - & - & 0.821 & - & - & - & \checkmark \\
\hline \cline{0-9}
\multicolumn{2}{|c|}{Weight} & 0.327 & 0.057 & 0.933 & 2.630 & \multicolumn{4}{c|}{$\sum W_\gamma=3.946$} \\
\cline{0-9}
\multicolumn{2}{|c|}{Confidence} & $8\%$ & $1\%$ & $24\%$ & $67\%$ & \multicolumn{4}{c}{} \\
\cline{0-5}
\end{tabular}
\label{table:example2}
\end{table*}

\begin{table*}
\caption{Digit 5 Outputs}
\centering\begin{tabular}{ | c ||  c | c | c | c | c | c | c | c | c |}
 digit 5 & $P_1$ & $P_2$ & $P_3$ & $P_4$ & $P_5$ & $P_6$ & $P_7$ & $P_8$ & $P_9$ \\
\hline \hline
effectiveness $E_{\pi,5}$  & 100.0 & 14.2 & 5.7 & 20.8 & 21.5 & 85.4 & 3.8 & 70.0 & 5.5 \\
\hline
$\sigma$ & 0.300& 0.076& 0.082& 0.068& 0.089& 0.252& 0.074& 0.211& 0.077 \\
\hline
k & 10.000& 3.995& -1.864& 5.555& 6.229& 9.550& -1.941& 9.779& -2.063 \\
\hline
skew & 3.162& 1.913& 0.330& 2.324& 2.413& 3.073& -0.025& 3.116& 0.061 \\
\hline
0 & 0.000 & 0.115 & 0.208 & 0.093 & 0.073 & 0.002 & 0.007 & 0.023 & 0.020 \\
\hline
1 & 0.000 & 0.048 & 0.223 & 0.057 & 0.044 & 0.111 & 0.193 & 0.008 & 0.183 \\
\hline
2 & 0.000 & 0.048 & 0.051 & 0.058 & 0.053 & 0.001 & 0.089 & 0.027 & 0.069 \\
\hline
3 & 0.000 & 0.162 & 0.082 & 0.154 & 0.154 & 0.002 & 0.149 & 0.079 & 0.178 \\
\hline
4 & 0.000 & 0.046 & 0.006 & 0.056 & 0.045 & 0.003 & 0.122 & 0.019 & 0.130 \\
\hline
5 & 1.000 & 0.300 & 0.200 & 0.284 & 0.345 & 0.849 & 0.186 & 0.732 & 0.185 \\
\hline
6 & 0.000 & 0.097 & 0.069 & 0.059 & 0.090 & 0.009 & 0.027 & 0.036 & 0.037 \\
\hline
7 & 0.000 & 0.045 & 0.167 & 0.067 & 0.041 & 0.001 & 0.185 & 0.011 & 0.212 \\
\hline
8 & 0.000 & 0.109 & 0.022 & 0.094 & 0.101 & 0.014 & 0.005 & 0.042 & 0.005 \\
\hline
9 & 0.000 & 0.044 & 0.019 & 0.069 & 0.043 & 0.009 & 0.032 & 0.034 & 0.028 \\
\hline
false positive  & 0.0 & 0.5 & 0.2 & 0.4 & 0.7 & 0.4 & 0.0 & 0.3 & 0.0 \\
\hline
false negative  & 0.0 & 84.8 & 94.1 & 78.8 & 78.3 & 13.4 & 96.2 & 29.2 & 94.5 \\
\hline
\end{tabular}
\label{table:digit5out}
\end{table*}

\begin{table*}
\caption{Digit 6 Outputs}
\centering\begin{tabular}{ | c ||  c | c | c | c | c | c | c | c | c |}
 digit 6 & $P_1$ & $P_2$ & $P_3$ & $P_4$ & $P_5$ & $P_6$ & $P_7$ & $P_8$ & $P_9$ \\
\hline \hline
effectiveness $E_{\pi,6}$  & 100.0 & 46.5 & 49.5 & 32.0 & 49.0 & 93.3 & 81.8 & 70.7 & 83.2 \\
\hline
$\sigma$ & 0.300& 0.112& 0.130& 0.094& 0.133& 0.264& 0.240& 0.209& 0.246 \\
\hline
k & 10.000& 8.507& 8.617& 9.864& 9.469& 9.955& 9.935& 9.889& 9.930 \\
\hline
skew & 3.162& 2.842& 2.886& 3.132& 3.048& 3.153& 3.148& 3.138& 3.147 \\
\hline
0 & 0.000 & 0.100 & 0.042 & 0.069 & 0.080 & 0.020 & 0.001 & 0.034 & 0.004 \\
\hline
1 & 0.000 & 0.052 & 0.050 & 0.066 & 0.048 & 0.005 & 0.035 & 0.011 & 0.033 \\
\hline
2 & 0.000 & 0.057 & 0.135 & 0.064 & 0.061 & 0.020 & 0.022 & 0.028 & 0.013 \\
\hline
3 & 0.000 & 0.095 & 0.046 & 0.070 & 0.079 & 0.002 & 0.027 & 0.051 & 0.032 \\
\hline
4 & 0.000 & 0.039 & 0.044 & 0.067 & 0.040 & 0.000 & 0.027 & 0.046 & 0.023 \\
\hline
5 & 0.000 & 0.101 & 0.065 & 0.052 & 0.086 & 0.008 & 0.028 & 0.029 & 0.024 \\
\hline
6 & 1.000 & 0.426 & 0.482 & 0.380 & 0.495 & 0.890 & 0.818 & 0.725 & 0.837 \\
\hline
7 & 0.000 & 0.057 & 0.049 & 0.077 & 0.047 & 0.000 & 0.033 & 0.007 & 0.038 \\
\hline
8 & 0.000 & 0.027 & 0.086 & 0.068 & 0.030 & 0.034 & 0.004 & 0.041 & 0.001 \\
\hline
9 & 0.000 & 0.029 & 0.026 & 0.077 & 0.038 & 0.000 & 0.007 & 0.027 & 0.005 \\
\hline
false positive  & 0.0 & 2.6 & 2.5 & 0.5 & 2.2 & 0.7 & 0.2 & 0.6 & 0.0 \\
\hline
false negative  & 0.0 & 53.3 & 50.1 & 67.5 & 50.7 & 6.6 & 18.1 & 28.1 & 16.8 \\
\hline
\end{tabular}
\label{table:digit6out}
\end{table*}

%\section{References}
\bibliographystyle{plain}
\bibliography{references}{}


\end{document}
